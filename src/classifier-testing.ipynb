{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Anar Amirli\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib  inline\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import itertools\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dataset(y_val): \n",
    "    heading = 'Pitch index counts'\n",
    "    print(heading + '\\n' + '-'*len(heading))\n",
    "    for key, val in sorted(Counter(y_val).items()):\n",
    "        print('{}\\t: {}'.format(int(key), val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/general/train/all_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labale and Columns Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Index: Null']\n",
    "for i in range(1,27):\n",
    "    classes.append('Index: ' + str(i))\n",
    "feature_names = list(train_data.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_cv(clf, X, y, feature_names, classes=None, k=10):\n",
    "    label_ids = sorted(np.unique(y), key=abs)\n",
    "    \n",
    "    # shuffle the data\n",
    "    X, y = shuffle(X, y)\n",
    "    \n",
    "    # get K folds\n",
    "    skf = KFold(n_splits=k, shuffle=True)\n",
    "    skf.get_n_splits(X, y)\n",
    "    \n",
    "    # cumulative confusion matrix and feature importance arrays\n",
    "    conf_mat = np.zeros((len(label_ids), len(label_ids)))\n",
    "    feature_importances = np.zeros(X.shape[1])\n",
    "    \n",
    "    # for each fold in KFold\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # fit with current train set\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        # predict the current test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # keep cumulative sum of conf_mat and feature_importances\n",
    "        conf_mat += confusion_matrix(y_test, y_pred, labels=label_ids)\n",
    "        feature_importances += clf.feature_importances_\n",
    "     \n",
    "    # normalize importances\n",
    "    feature_importances /= k\n",
    "    \n",
    "    if classes is None:\n",
    "        classes = label_ids\n",
    "    \n",
    "    # plot\n",
    "    plot_confusion_matrix(conf_mat, classes, 'Prediction', 'Truth', normalize=True)\n",
    "    plot_hbar_nameval(feature_names, feature_importances, 'Feature importances', max_bars=20)\n",
    "    return conf_mat, feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "SMOTE (Synthetic Minority Over-Sampling Technique) is an over-sampling technique that introduces small perturbations to synthetic examples along the direction of existing samples to reduce overfitting. See original paper for detailed explanation of SMOTE.\n",
    "## SMOTE Implementation\n",
    "There is a SMOTE implementation in imblearn package for scikit-learn. However, there is not an option to apply SMOTE with arbitrary percentages (SMOTE-100, SMOTE-300, etc.); it simply balances all the classes. And also since SMOTE is not a hard to implement algorithm, we provide our own implementation. Our dataset sizes are not big (order of 10k). Hence, a simple Python implementation would be more than sufficien for our needs. See the original paper for reference algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def smote(samples, amount, k=5):\n",
    "    \"\"\"\n",
    "    Apply SMOTE algorithm to samples and return a new samples\n",
    "    array with synthetically created samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    samples: (n_samples, n_features) samples array to be sent to\n",
    "             SMOTE algorithm.\n",
    "    amount: Percentage of newly created synthetic samples. (E.g.\n",
    "            amount=100 would create as many synthetic examples\n",
    "            as existing ones).\n",
    "    k: Number of nearest neighbors in SMOTE algorithm.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out: ((1 + amount/100)*n_samples, n_features) samples array containing\n",
    "         the original and the newly created synthetic examples.\n",
    "         \n",
    "    References\n",
    "    ----------\n",
    "    http://www.jair.org/media/953/live-953-2037-jair.pdf\n",
    "    \"\"\"\n",
    "    samples = np.copy(samples)\n",
    "    n_samples, n_features = samples.shape\n",
    "    # handle amount < 100 case\n",
    "    if amount < 100:\n",
    "        num_samples = int(len(samples)*(amount/100))\n",
    "        np.shuffle(samples)\n",
    "        samples = samples[:num_samples, :]\n",
    "        amount = 100\n",
    "    amount = int(amount/100)\n",
    "    synthetic = np.empty((n_samples*amount, n_features))\n",
    "    # find k nearest neighbors of each point and store it in nnarray\n",
    "    nbrs = NearestNeighbors(n_neighbors=k + 1).fit(samples)\n",
    "    _, nnarray = nbrs.kneighbors(samples)\n",
    "    nnarray = nnarray[:, 1:]  # get rid of self-nearest-neighbor.\n",
    "    # create synthetic examples and store them in synthetic.\n",
    "    for i, neighbors in enumerate(nnarray):\n",
    "        for j in range(amount):\n",
    "            chosen = neighbors[randint(0, k - 1)]\n",
    "            diff = samples[chosen] - samples[i]\n",
    "            gap = np.random.rand(n_features)\n",
    "            synthetic[i*amount + j] = samples[i] + gap*diff\n",
    "    out = np.vstack((samples, synthetic))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch index counts\n",
      "------------------\n",
      "0\t: 10335\n",
      "1\t: 4307\n",
      "2\t: 11274\n",
      "3\t: 4400\n",
      "4\t: 12094\n",
      "5\t: 5516\n",
      "6\t: 7694\n",
      "7\t: 5676\n",
      "8\t: 8744\n",
      "9\t: 10696\n",
      "10\t: 8706\n",
      "11\t: 11007\n",
      "12\t: 12279\n",
      "13\t: 11382\n",
      "14\t: 10812\n",
      "15\t: 9859\n",
      "16\t: 11316\n",
      "17\t: 9124\n",
      "18\t: 7108\n",
      "19\t: 9729\n",
      "20\t: 7468\n",
      "21\t: 5346\n",
      "22\t: 8589\n",
      "23\t: 6858\n",
      "24\t: 3656\n",
      "25\t: 8170\n",
      "26\t: 1320\n"
     ]
    }
   ],
   "source": [
    "describe_dataset(train_data.values[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE for  Segmnet 16\n",
    "\n",
    "Assigment number of segment 16 in our dataset is much fewer than other segments, 1320. We use SMOTE algorithm with various synthetic example amounts to generate more segment 16 examples.\n",
    "\n",
    "We can't use nominal features in our current SMOTE implementation\n",
    "\n",
    "TODO: Implement SMOTE-NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch index counts\n",
      "------------------\n",
      "0\t: 10335\n",
      "1\t: 4307\n",
      "2\t: 11274\n",
      "3\t: 4400\n",
      "4\t: 12094\n",
      "5\t: 5516\n",
      "6\t: 7694\n",
      "7\t: 5676\n",
      "8\t: 8744\n",
      "9\t: 10696\n",
      "10\t: 8706\n",
      "11\t: 11007\n",
      "12\t: 12279\n",
      "13\t: 11382\n",
      "14\t: 10812\n",
      "15\t: 9859\n",
      "16\t: 11316\n",
      "17\t: 9124\n",
      "18\t: 7108\n",
      "19\t: 9729\n",
      "20\t: 7468\n",
      "21\t: 5346\n",
      "22\t: 8589\n",
      "23\t: 6858\n",
      "24\t: 3656\n",
      "25\t: 8170\n",
      "26\t: 2640\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def smote_sampling(X, y, pitch_segment):\n",
    "    pitch_segment = 26\n",
    "    smote_amount = 100\n",
    "    pitch_segment_mask = y == pitch_segment\n",
    "    \n",
    "    penalty_features = X[pitch_segment_mask]\n",
    "    synthetic_penalty = smote(penalty_features, smote_amount)\n",
    "    n_synthetic = len(synthetic_penalty)\n",
    "\n",
    "    # merge synthetic examples with original examples\n",
    "    X_out = np.vstack((X[~pitch_segment_mask], synthetic_penalty))\n",
    "    y_out = np.concatenate((y[~pitch_segment_mask], [pitch_segment]*n_synthetic))\n",
    "\n",
    "    return X_out, y_out\n",
    "\n",
    "x_train = train_data.values[:, 0:-1]\n",
    "y_train = train_data.values[:, -1]\n",
    "\n",
    "X_out, y_out = smote_sampling(x_train, y_train,26)\n",
    "\n",
    "describe_dataset(y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Fold Cross Validation on Training Data\n",
    "In this section, we test our model parameters using cross validation on training data. Classifier used in this section is not previously trained. Hence, it is not our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(\n",
    "    n_estimators=128,\n",
    "    criterion='gini',\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=6,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced_subsample'\n",
    ")\n",
    "cm, fi = random_forest_cv(\n",
    "    clf,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    feature_names,\n",
    "    classes=classes,\n",
    "    k=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
